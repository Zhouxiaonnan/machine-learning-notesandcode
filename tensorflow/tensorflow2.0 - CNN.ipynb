{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "### 1. è‡ªå®šä¹‰æƒå€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2, 5, 5, 3]) # ä¸¤å¼ 5*5çš„å›¾ç‰‡ï¼ŒRGBä¸‰ä¸ªé€šé“\n",
    "w = tf.random.normal([3, 3, 3, 4]) # 4ä¸ª3*3ä¸”3ä¸ªé€šé“çš„å·ç§¯æ ¸\n",
    "\n",
    "# è¿›è¡Œå·ç§¯è®¡ç®—ï¼Œæ­¥é•¿ä½1ï¼Œpaddingä¸º0\n",
    "out = tf.nn.conv2d(x, w, strides = 1, padding = [[0,0],[0,0],[0,0],[0,0]])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢å¾—åˆ°çš„outçš„å½¢çŠ¶ï¼Œç¬¬ä¸€ä¸ª2ä»£è¡¨ä¸¤å¼ å›¾ç‰‡ï¼Œç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ª3ä»£è¡¨å¾—åˆ°ç»è¿‡å·ç§¯æ ¸è¿ç®—åå¾—åˆ°çš„çŸ©é˜µå¤§å°ï¼Œæœ€åä¸€ä¸ª4æ˜¯å› ä¸ºä¸€å…±æœ‰4ä¸ªå·ç§¯æ ¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æˆ‘ä»¬è®©æ¯å¼ å›¾ç‰‡çš„ä¸Šä¸‹å·¦å³éƒ½å¡«å……ä¸€ä¸ªæ­¥é•¿\n",
    "out = tf.nn.conv2d(x, w, strides = 1, padding = [[0,0],[1,1],[1,1],[0,0]])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æˆ‘ä»¬ä¹Ÿå¯ä»¥è®©padding = sameï¼Œè¿™æ ·tensorflowè‡ªåŠ¨è¿ç®—å®Œæˆå¡«å……\n",
    "out = tf.nn.conv2d(x, w, strides = 1, padding = 'SAME')\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹äºä¸€å¼ å˜åŒ–ä¸æ˜¯å¾ˆå¤§çš„å›¾ç‰‡ï¼Œæ­¥é•¿å¯ä»¥è®¾ç½®çš„å¤§ä¸€äº›\n",
    "out = tf.nn.conv2d(x, w, strides = 3, padding = 'SAME')\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœéœ€è¦åç½®åˆ™éœ€è¦æ‰‹åŠ¨æ·»åŠ åç½®\n",
    "b = tf.zeros([4])\n",
    "out = out + b # è¿™é‡Œè‡ªåŠ¨ä½¿ç”¨å¹¿æ’­æ³•åˆ™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. å·ç§¯å±‚ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Conv2D(4, # å·ç§¯æ ¸çš„æ•°é‡ \n",
    "                               kernel_size = 3,  # å·ç§¯æ ¸çš„å½¢çŠ¶ä¸º3*3\n",
    "                               strides = 1,  # æ­¥é•¿\n",
    "                               padding = 'SAME' # å¡«å……\n",
    "                              )\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥æŸ¥çœ‹å¾…ä¼˜åŒ–å¼ é‡åˆ—è¡¨ï¼ŒåŒ…æ‹¬æƒå€¼çŸ©é˜µWå’Œåç½®b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹å¾…ä¼˜åŒ–å‚æ•°\n",
    "# layer.trainable_variables\n",
    "# layer.kernel\n",
    "# layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. å·ç§¯ç¥ç»ç½‘ç»œç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = Sequential([\n",
    "    layers.Conv2D(6, kernel_size = 3, strides = 1, padding = 'SAME'), # å·ç§¯å±‚\n",
    "    layers.MaxPooling2D(pool_size = 2, strides = 2), # æ± åŒ–å±‚\n",
    "    layers.ReLU(), # æ¿€æ´»å‡½æ•°\n",
    "    layers.Conv2D(16, kernel_size = 3, strides = 1),\n",
    "    layers.MaxPooling2D(pool_size = 2, strides = 2),\n",
    "    layers.ReLU(),\n",
    "    layers.Flatten(), # æ‰“å¹³å±‚ï¼Œæ–¹ä¾¿å…¨è¿æ¥å¤„ç†\n",
    "    layers.Dense(120, activation = 'relu'),\n",
    "    layers.Dense(60, activation = 'relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "CNN.build(input_shape = (4, 28, 28, 1))\n",
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, optimizers\n",
    "# ä½¿ç”¨mnistæ•°æ®é›†\n",
    "criteon = losses.CategoricalCrossentropy(from_logits = True)\n",
    "with tf.GradientTape() as tape:\n",
    "    x = tf.expand_dims(x, axis = 3)\n",
    "    out = CNN(x)\n",
    "    y_onehot = tf.one_hot(y, depth = 10)\n",
    "    loss = criteon(y_onehot, out)\n",
    "grads = tape.gradient(loss, CNN.trainable_variables)\n",
    "optimizers.apply_gradients(zip(grads, CNN.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. è¡¨ç¤ºå­¦ä¹ \n",
    "é€šè¿‡å°†æ¯å±‚çš„ç‰¹å¾å›¾åˆ©ç”¨åå·ç§¯ç½‘ç»œ(DeconvolutionalNetwork)æ˜ å°„å›è¾“å…¥å›¾ç‰‡ï¼Œå³å¯æŸ¥çœ‹å­¦åˆ°çš„ç‰¹å¾åˆ†å¸ƒï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ï¼Œç¬¬äºŒå±‚çš„ç‰¹å¾å¯¹åº”åˆ°è¾¹ã€è§’ã€è‰²å½©ç­‰åº•å±‚å›¾åƒæå–ï¼›ç¬¬ä¸‰å±‚å¼€å§‹æ•è·åˆ°çº¹ç†è¿™äº›ä¸­å±‚ç‰¹å¾ï¼›ç¬¬å››ã€äº”å±‚å‘ˆç°äº†ç‰©ä½“çš„éƒ¨åˆ†ç‰¹å¾ï¼Œå¦‚å°ç‹—çš„è„¸éƒ¨ï¼Œé¸Ÿç±»çš„è„šéƒ¨ç­‰é«˜å±‚ç‰¹å¾ã€‚ é€šè¿‡è¿™äº›å¯è§†åŒ–çš„æ‰‹æ®µï¼Œæˆ‘ä»¬å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šæ„Ÿå—å·ç§¯ç¥ç»ç½‘ç»œçš„ç‰¹å¾å­¦ä¹ è¿‡ç¨‹ã€‚\n",
    "\n",
    "å›¾ç‰‡æ•°æ®çš„è¯†åˆ«è¿‡ç¨‹ä¸€èˆ¬è®¤ä¸ºä¹Ÿæ˜¯è¡¨ç¤ºå­¦ä¹ (Representation Learning)çš„è¿‡ç¨‹ï¼Œä»æ¥å—åˆ°çš„åŸå§‹åƒç´ å¼€å§‹ï¼Œé€æ¸æå–è¾¹ç¼˜ã€è§’ç‚¹ç­‰åº•å±‚ç‰¹å¾ï¼Œå†åˆ°çº¹ç†ç­‰ä¸­å±‚ç‰¹å¾ï¼Œå†åˆ°å¤´éƒ¨ã€ç‰©ä½“éƒ¨ä»¶ç­‰é«˜å±‚ç‰¹å¾ï¼Œæœ€åçš„ç½‘ç»œå±‚åŸºäºè¿™äº›å­¦ä¹ åˆ°çš„æŠ½è±¡ç‰¹å¾è¡¨ç¤º(Representation)åšåˆ†ç±»é€»è¾‘çš„å­¦ä¹ ã€‚å­¦ä¹ åˆ°çš„ç‰¹å¾è¶Šé«˜å±‚ï¼Œè¶Šå‡†ç¡®ï¼Œå°±è¶Šæœ‰åˆ©äºåˆ†ç±»å™¨çš„åˆ†ç±»ï¼Œä»è€Œè·å¾—è¾ƒå¥½çš„æ€§èƒ½ã€‚ä»è¡¨ç¤ºå­¦ä¹ çš„è§’åº¦æ¥ç†è§£ï¼Œå·ç§¯ç¥ç»ç½‘ç»œé€šè¿‡å±‚å±‚å †å æ¥é€å±‚æå–ç‰¹å¾ï¼Œç½‘ç»œè®­ç»ƒçš„è¿‡ç¨‹å¯ä»¥çœ‹æˆç‰¹å¾çš„å­¦ä¹ è¿‡ç¨‹ï¼ŒåŸºäºå­¦ä¹ åˆ°çš„é«˜å±‚æŠ½è±¡ç‰¹å¾å¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œåˆ†ç±»ä»»åŠ¡ã€‚\n",
    "\n",
    "åº”ç”¨è¡¨ç¤ºå­¦ä¹ çš„æ€æƒ³ï¼Œè®­ç»ƒå¥½çš„å·ç§¯ç¥ç»ç½‘ç»œå¾€å¾€èƒ½å¤Ÿå­¦ä¹ åˆ°è¾ƒå¥½çš„ç‰¹å¾ï¼Œè¿™ç§ç‰¹å¾çš„æå–æ–¹æ³•ä¸€èˆ¬æ˜¯é€šç”¨çš„ã€‚æ¯”å¦‚åœ¨çŒ«ã€ ç‹—ä»»åŠ¡ä¸Šå­¦ä¹ åˆ°å¤´ã€è„šã€èº«èº¯ç­‰ç‰¹å¾çš„è¡¨ç¤ºï¼Œåœ¨å…¶ä»–åŠ¨ç‰©ä¸Šä¹Ÿèƒ½å¤Ÿä¸€å®šç¨‹åº¦ä¸Šä½¿ç”¨ã€‚åŸºäºè¿™ç§æ€æƒ³ï¼Œå¯ä»¥å°†åœ¨ä»»åŠ¡ A ä¸Šè®­ç»ƒå¥½çš„æ·±å±‚ç¥ç»ç½‘ç»œçš„å‰é¢æ•°ä¸ªç‰¹å¾æå–å±‚è¿ç§»åˆ°ä»»åŠ¡ B ä¸Šï¼Œåªéœ€è¦è®­ç»ƒä»»åŠ¡ B çš„åˆ†ç±»é€»è¾‘(è¡¨ç°ä¸ºç½‘ç»œçš„æœ€æœ«æ•°å±‚)ï¼Œå³å¯å–å¾—éå¸¸å¥½çš„æ•ˆæœï¼Œè¿™ç§æ–¹å¼æ˜¯è¿ç§»å­¦ä¹ çš„ä¸€ç§ï¼Œä»ç¥ç»ç½‘ç»œè§’åº¦ä¹Ÿç§°ä¸ºç½‘ç»œå¾®è°ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CNNç½‘ç»œçš„æ¢¯åº¦ä¼ æ’­\n",
    "é€šè¿‡å¾ªç¯ç§»åŠ¨æ„Ÿå—é‡çš„æ–¹å¼å¹¶æ²¡æœ‰æ”¹å˜ç½‘ç»œå±‚å¯å¯¼æ€§ï¼ŒåŒæ—¶æ¢¯åº¦çš„æ¨å¯¼ä¹Ÿå¹¶ä¸å¤æ‚ï¼Œåªæ˜¯å½“ç½‘ç»œå±‚æ•°å¢å¤§ä»¥åï¼Œäººå·¥æ¢¯åº¦æ¨å¯¼å°†å˜å¾—ååˆ†çš„ç¹çã€‚\n",
    "\n",
    "### 6. æ± åŒ–å±‚\n",
    "æ± åŒ–å±‚åŒæ ·åŸºäºå±€éƒ¨ç›¸å…³æ€§çš„æ€æƒ³ï¼Œé€šè¿‡ä»å±€éƒ¨ç›¸å…³çš„ä¸€ç»„å…ƒç´ ä¸­è¿›è¡Œé‡‡æ ·æˆ–ä¿¡æ¯èšåˆï¼Œä»è€Œå¾—åˆ°æ–°çš„å…ƒç´ å€¼ã€‚ç‰¹åˆ«åœ°ï¼Œæœ€å¤§æ± åŒ–å±‚(Max Pooling)ä»å±€éƒ¨ç›¸å…³å…ƒç´ é›†ä¸­é€‰å–æœ€å¤§çš„ä¸€ä¸ªå…ƒç´ å€¼ï¼Œå¹³å‡æ± åŒ–å±‚(Average Pooling)ä»å±€éƒ¨ç›¸å…³å…ƒç´ é›†ä¸­è®¡ç®—å¹³å‡å€¼å¹¶è¿”å›ã€‚\n",
    "\n",
    "ç”±äºæ± åŒ–å±‚æ²¡æœ‰éœ€è¦å­¦ä¹ çš„å‚æ•°ï¼Œè®¡ç®—ç®€å•ï¼Œå¯ä»¥æœ‰æ•ˆå‡ä½ç‰¹å¾å›¾çš„å°ºå¯¸ï¼Œéå¸¸é€‚åˆå›¾ç‰‡è¿™ç§ç±»å‹çš„æ•°æ®ã€‚\n",
    "\n",
    "### 7. BatchNormå±‚\n",
    "å·ç§¯ç¥ç»ç½‘ç»œçš„å‡ºç°ï¼Œç½‘ç»œå‚æ•°é‡å¤§å¤§å‡ä½ï¼Œä½¿å¾—å‡ åå±‚çš„æ·±å±‚ç½‘ç»œæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œåœ¨æ®‹å·®ç½‘ç»œå‡ºç°ä¹‹å‰ï¼Œç½‘ç»œçš„åŠ æ·±ä½¿å¾—ç½‘ç»œè®­ç»ƒå˜å¾—éå¸¸ä¸ç¨³å®šï¼Œ ç”šè‡³å‡ºç°ç½‘ç»œé•¿æ—¶é—´ä¸æ›´æ–°æˆ–è€…ä¸æ”¶æ•›çš„æƒ…å½¢ï¼ŒåŒæ—¶ç½‘ç»œå¯¹è¶…å‚æ•°æ¯”è¾ƒæ•æ„Ÿï¼Œè¶…å‚æ•°çš„å¾®é‡æ‰°åŠ¨ä¹Ÿä¼šå¯¼è‡´ç½‘ç»œçš„è®­ç»ƒè½¨è¿¹å®Œå…¨æ”¹å˜ã€‚\n",
    "\n",
    "BNå±‚æ˜¯ä¸€ç§å‚æ•°æ ‡å‡†åŒ–çš„æ‰‹æ®µï¼Œå®ƒèƒ½è®©æ”¶æ•›æ›´å¿«ï¼Œå‚æ•°è®¾å®šæ›´åŠ è‡ªç”±çµæ´»ã€‚\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨Sigmoidå‡½æ•°æ—¶ï¼Œä¸ºäº†é¿å…æ¢¯åº¦å¼¥æ•£ï¼Œç»å¸¸éœ€è¦å°†è¾“å…¥å‚æ•°è¿›è¡Œæ ‡å‡†åŒ–ï¼Œæ ‡å‡†åŒ–å°†å€¼æ˜ å°„åœ¨0é™„è¿‘ï¼Œæ­¤å¤„çš„å¯¼æ•°ä¸è‡³äºè¿‡å°ï¼Œå› æ­¤ä¸å®¹æ˜“å‡ºç°æ¢¯åº¦å¼¥æ•£çš„ç°è±¡ã€‚\n",
    "\n",
    "æˆ‘ä»¬èƒ½å¤Ÿç»éªŒæ€§å½’çº³å‡ºï¼šç½‘ç»œå±‚è¾“å…¥ğ‘¥åˆ†å¸ƒç›¸è¿‘ï¼Œ å¹¶ä¸”åˆ†å¸ƒåœ¨è¾ƒå°èŒƒå›´å†…æ—¶(å¦‚ 0 é™„è¿‘)ï¼Œæ›´æœ‰åˆ©äºå‡½æ•°çš„ä¼˜åŒ–ã€‚ \n",
    "\n",
    "æ ‡å‡†åŒ–è¿ç®—ä¸å¼•å…¥é¢å¤–çš„å‚æ•°ï¼Œå› ä¸ºå‡å€¼å’Œæ ‡å‡†å·®éƒ½å¯ä»¥é€šè¿‡ç»Ÿè®¡å¾—åˆ°ã€‚\n",
    "\n",
    "ä¸ºäº†æé«˜BNçš„è¡¨è¾¾èƒ½åŠ›ï¼Œåˆå¼•å…¥äº†scale and shiftæŠ€å·§ï¼Œå³å°†æ ‡å‡†åŒ–åçš„è¾“å…¥å†æ¬¡è¿›è¡Œç¼©æ”¾å’Œå¹³ç§»ï¼Œä½†æ˜¯æ­¤æ—¶æ§åˆ¶ç¼©æ”¾å’Œå¹³ç§»çš„å‚æ•°æ˜¯éœ€è¦ç»è¿‡åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œè‡ªåŠ¨ä¼˜åŒ–çš„ï¼Œè¿™æ ·å¯ä»¥å®ç°ç½‘ç»œå±‚æŒ‰éœ€ç¼©æ”¾å’Œå¹³ç§»æ•°æ®åˆ†å¸ƒçš„ç›®çš„ã€‚\n",
    "\n",
    "è€Œæ ‡å‡†åŒ–ä¹Ÿåˆ†ä¸ºå‡ ç§ï¼š\n",
    "- Batch Norm\n",
    "- Layer Normï¼šç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®\n",
    "- Instance Normï¼šç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªé€šé“ä¸Šç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®\n",
    "- Group Normï¼šå°†cé€šé“åˆ†æˆè‹¥å¹²ç»„ï¼Œç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„é€šé“ç»„å†…çš„ç‰¹å¾å‡å€¼å’Œæ–¹å·®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºBNå±‚\n",
    "layer = layers.BatchNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸å…¨è¿æ¥å±‚ã€ å·ç§¯å±‚ä¸åŒï¼Œ BN å±‚çš„è®­ç»ƒé˜¶æ®µå’Œæµ‹è¯•é˜¶æ®µçš„è¡Œä¸ºä¸åŒï¼Œéœ€è¦é€šè¿‡è®¾ç½®training æ ‡å¿—ä½æ¥åŒºåˆ†è®­ç»ƒæ¨¡å¼è¿˜æ˜¯æµ‹è¯•æ¨¡å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # åœ¨å·ç§¯å±‚åæ·»åŠ BNå±‚\n",
    "# network = Sequential([\n",
    "#     layers.Conv2D(6, kernel_size = 3, strides = 1),\n",
    "#     # æ’å…¥BNå±‚\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.MaxPooling2D(pool_size = 2, strides = 2)\n",
    "#     # ...\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # åœ¨è®­ç»ƒé˜¶æ®µï¼Œéœ€è¦è®¾ç½®ç½‘ç»œçš„å‚æ•°training = Trueä»¥åŒºåˆ†BNå±‚æ˜¯è®­ç»ƒè¿˜æ˜¯æµ‹è¯•æ¨¡å‹\n",
    "# with tf.GradientTape() as tape:\n",
    "#     x = tf.expand_dims(x, axis = 3)\n",
    "#     out = network(x, training = True) # è®¾ç½®training = Trueï¼ŒåŒºåˆ†BNå±‚æ˜¯è®­ç»ƒè¿˜æ˜¯æµ‹è¯•æ¨¡å‹\n",
    "\n",
    "# # åœ¨æµ‹è¯•é˜¶æ®µï¼Œè®¾ç½®training = Falseï¼Œé¿å…BNå±‚é‡‡ç”¨é”™è¯¯çš„è¡Œä¸ºï¼š\n",
    "# for x, y in db_test:\n",
    "#     x = tf.expand_dims(x, axis = 3)\n",
    "#     out = network(x, training = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ç»å…¸CNN\n",
    "#### AlexNet\n",
    "- å±‚æ•°è¾¾åˆ°8å±‚\n",
    "- é‡‡ç”¨äº†ReLUæ¿€æ´»å‡½æ•°\n",
    "- å¼•å…¥äº†Dropout\n",
    "\n",
    "#### VGGç³»åˆ—\n",
    "- å±‚æ•°æå‡åˆ°19å±‚\n",
    "- é‡‡ç”¨æ›´å°çš„3\\*3å·ç§¯æ ¸ï¼ŒAlexNetå·ç§¯æ ¸ä¸º7\\*7\n",
    "- é‡‡ç”¨æ›´å°çš„æ± åŒ–å±‚2\\*2çª—å£å’Œæ­¥é•¿s=2ï¼ŒAlexNetæ­¥é•¿ä¸º2ï¼Œæ± åŒ–çª—å£ä¸º3\\*3\n",
    "\n",
    "#### GoogleNet\n",
    "- 22å±‚\n",
    "- ä½¿ç”¨äº†1\\*1çš„æœ€å°å·ç§¯å±‚\n",
    "- é‡‡ç”¨äº†æ¨¡å—åŒ–è®¾è®¡çš„æ€æƒ³\n",
    "\n",
    "### 9. CIFAR10ä¸VGG13å®æˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–data\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ é™¤yçš„ä¸€ä¸ªç»´åº¦\n",
    "train_y = tf.squeeze(train_y, axis = 1)\n",
    "test_y = tf.squeeze(test_y, axis = 1)\n",
    "\n",
    "# æ‰“å°è®­ç»ƒé›†å’Œæµ‹è¯•é›†å½¢çŠ¶\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # è‡ªå®šä¹‰çš„é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess(x, y):\n",
    "    # è°ƒç”¨æ­¤å‡½æ•°æ—¶ä¼šè‡ªåŠ¨ä¼ å…¥ x,y å¯¹è±¡\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255. # æ ‡å‡†åŒ–åˆ° 0~1\n",
    "    y = tf.cast(y, dtype=tf.int32) # è½¬æˆæ•´å½¢å¼ é‡\n",
    "    y = tf.one_hot(y, depth=10) # one-hot ç¼–ç \n",
    "    # è¿”å›çš„ x,y å°†æ›¿æ¢ä¼ å…¥çš„ x,y å‚æ•°ï¼Œä»è€Œå®ç°æ•°æ®çš„é¢„å¤„ç†åŠŸèƒ½\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºè®­ç»ƒé›†å¯¹è±¡\n",
    "# from_tensor_sliceså¯ä»¥è®©æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ä¸æ ‡ç­¾ä¸€ä¸€å¯¹åº”\n",
    "train_db = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "\n",
    "# mapå‡½æ•°è°ƒç”¨preprocessï¼Œè¿›è¡Œé¢„å¤„ç†ï¼Œbatchæ˜¯åˆ†æ‰¹ï¼Œæ¯æ‰¹128ä¸ªæ ·æœ¬\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºæµ‹è¯•é›†å¯¹è±¡\n",
    "test_db = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "test_db = test_db.map(preprocess).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»è®­ç»ƒé›†ä¸­é‡‡æ ·ä¸€ä¸ªBatchï¼Œå¹¶è§‚å¯Ÿ\n",
    "sample = next(iter(train_db))\n",
    "print('sample:', sample[0].shape, sample[1].shape,\n",
    "tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG13åº”å½“æœ‰13å±‚ï¼Œè¿™é‡Œåªæ˜¯ç¤ºä¾‹\n",
    "CNN = Sequential([\n",
    "    layers.Conv2D(1, kernel_size = [3, 3], padding = 'SAME', activation = 'relu'),\n",
    "    layers.Conv2D(1, kernel_size = [3, 3], padding = 'SAME', activation = 'relu'),\n",
    "    layers.MaxPool2D(pool_size = [2, 2], strides = 2, padding = 'SAME'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation = 'relu'),\n",
    "    layers.Dense(128, activation = 'relu'),\n",
    "    layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN.build(input_shape = [4, 32, 32, 3])\n",
    "CNN.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNN.fit(train_db, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. å·ç§¯å±‚å˜ç§\n",
    "#### ç©ºæ´å·ç§¯\n",
    "æ™®é€šçš„å·ç§¯å±‚ä¸ºäº†å‡å°‘ç½‘ç»œçš„å‚æ•°é‡ï¼Œå·ç§¯æ ¸çš„è®¾è®¡é€šå¸¸é€‰æ‹©è¾ƒå°çš„ 1x1,3x3 æ„Ÿå—é‡å¤§å°ã€‚å°å·ç§¯æ ¸ä½¿å¾—ç½‘ç»œæå–ç‰¹å¾æ—¶çš„æ„Ÿå—é‡åŒºåŸŸæœ‰é™ï¼Œä½†æ˜¯å¢å¤§æ„Ÿå—é‡çš„åŒºåŸŸåˆä¼šå¢åŠ ç½‘ç»œçš„å‚æ•°é‡å’Œè®¡ç®—ä»£ä»·ï¼Œå› æ­¤éœ€è¦æƒè¡¡è®¾è®¡ã€‚\n",
    "\n",
    "ç©ºæ´å·ç§¯(Dilated/Atrous Convolution)çš„æå‡ºè¾ƒå¥½åœ°è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç©ºæ´å·ç§¯åœ¨æ™®é€šå·ç§¯çš„æ„Ÿå—é‡ä¸Šå¢åŠ ä¸€ä¸ª dilation rate å‚æ•°ï¼Œç”¨äºæ§åˆ¶æ„Ÿå—é‡åŒºåŸŸçš„é‡‡æ ·æ­¥é•¿ã€‚\n",
    "\n",
    "å½“æ„Ÿå—é‡çš„é‡‡æ ·æ­¥é•¿ dilation rate ä¸º 1 æ—¶ï¼Œæ¯ä¸ªæ„Ÿå—é‡é‡‡æ ·ç‚¹ä¹‹é—´çš„è·ç¦»ä¸º 1ï¼Œæ­¤æ—¶çš„ç©ºæ´å·ç§¯é€€åŒ–ä¸ºæ™®é€šçš„å·ç§¯ï¼› ä½† dilation rate ä¸º 2 æ—¶ï¼Œæ„Ÿå—é‡æ¯ 2 ä¸ªå•å…ƒé‡‡æ ·ä¸€ä¸ªç‚¹ï¼Œæ¯ä¸ªé‡‡æ ·æ ¼å­ä¹‹é—´çš„è·ç¦»ä¸º 2ï¼›åŒæ ·çš„æ–¹æ³•ï¼Œdilation rate ä¸º 3ï¼Œ é‡‡æ ·æ­¥é•¿ä¸º 3ã€‚ å°½ç®¡ dilation rate çš„å¢å¤§ä¼šä½¿å¾—æ„Ÿå—é‡åŒºåŸŸå¢å¤§ï¼Œä½†æ˜¯å®é™…å‚ä¸è¿ç®—çš„ç‚¹æ•°ä»ç„¶ä¿æŒä¸å˜ã€‚\n",
    "\n",
    "ç©ºæ´å·ç§¯åœ¨ä¸å¢åŠ ç½‘ç»œå‚æ•°çš„æ¡ä»¶ä¸‹ï¼Œæä¾›äº†æ›´å¤§çš„æ„Ÿå—é‡çª—å£ã€‚ ä½†æ˜¯åœ¨ä½¿ç”¨ç©ºæ´å·ç§¯è®¾ç½®ç½‘ç»œæ¨¡å‹æ—¶ï¼Œéœ€è¦ç²¾å¿ƒè®¾è®¡ dilation rate å‚æ•°æ¥é¿å…å‡ºç°ç½‘æ ¼æ•ˆåº”ï¼Œ åŒæ—¶è¾ƒå¤§çš„dilation rate å‚æ•°å¹¶ä¸åˆ©äºå°ç‰©ä½“çš„æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç©ºæ´å·ç§¯å±‚\n",
    "x = tf.random.normal([1, 7, 7, 1])\n",
    "\n",
    "# dilation_rate\n",
    "layer = tf.keras.layers.Conv2D(1, kernel_size = 3, strides = 1, dilation_rate = 2)\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è½¬ç½®å·ç§¯\n",
    "è½¬ç½®å·ç§¯é€šè¿‡åœ¨è¾“å…¥ä¹‹é—´å¡«å……å¤§é‡çš„ padding æ¥å®ç°è¾“å‡ºé«˜å®½å¤§äºè¾“å…¥é«˜å®½çš„æ•ˆæœï¼Œä»è€Œå®ç°å‘ä¸Šé‡‡æ ·çš„ç›®çš„ã€‚\n",
    "\n",
    "è½¬ç½®å·ç§¯çš„ç›®çš„ï¼š\n",
    "- CNNå¯è§†åŒ–ï¼Œé€šè¿‡åå·ç§¯å°†å·ç§¯å¾—åˆ°çš„feature mapè¿˜åŸåˆ°åƒç´ ç©ºé—´ï¼Œæ¥è§‚å¯Ÿfeature mapå¯¹å“ªäº›patternç›¸åº”æœ€å¤§ï¼Œå³å¯è§†åŒ–å“ªäº›ç‰¹å¾æ˜¯å·ç§¯æ“ä½œæå–å‡ºæ¥çš„ï¼›\n",
    "- FCNå…¨å·ç§¯ç½‘ç»œä¸­ï¼Œç”±äºè¦å¯¹å›¾åƒè¿›è¡Œåƒç´ çº§çš„åˆ†å‰²ï¼Œéœ€è¦å°†å›¾åƒå°ºå¯¸è¿˜åŸåˆ°åŸæ¥çš„å¤§å°ï¼Œç±»ä¼¼upsamplingçš„æ“ä½œï¼Œæ‰€ä»¥éœ€è¦é‡‡ç”¨åå·ç§¯ï¼›\n",
    "- GANå¯¹æŠ—å¼ç”Ÿæˆç½‘ç»œä¸­ï¼Œç”±äºéœ€è¦ä»è¾“å…¥å›¾åƒåˆ°ç”Ÿæˆå›¾åƒï¼Œè‡ªç„¶éœ€è¦å°†æå–çš„ç‰¹å¾å›¾è¿˜åŸåˆ°å’ŒåŸå›¾åŒæ ·å°ºå¯¸çš„å¤§å°ï¼Œå³ä¹Ÿéœ€è¦åå·ç§¯æ“ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½¬ç½®å·ç§¯\n",
    "x = tf.random.normal([1, 2, 2, 1])\n",
    "w = tf.constant([[1, 2, 3.], [4, 5, 6], [7, 8, 9]])\n",
    "w = tf.expand_dims(w, axis = 2)\n",
    "w = tf.expand_dims(w, axis = 3)\n",
    "xx = tf.nn.conv2d_transpose(x, w, strides = 2, padding = 'VALID', \n",
    "                                          output_shape = [1, 5, 5, 1])\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½¬ç½®å·ç§¯å±‚\n",
    "layer = tf.keras.layers.Conv2DTranspose(1, kernel_size = 3, \n",
    "                                        strides = 1, \n",
    "                                        padding = 'VALID'\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### åˆ†ç¦»å·ç§¯\n",
    "æ™®é€šå·ç§¯åœ¨å¯¹å¤šé€šé“è¾“å…¥è¿›è¡Œè¿ç®—æ—¶ï¼Œå·ç§¯æ ¸çš„æ¯ä¸ªé€šé“ä¸è¾“å…¥çš„æ¯ä¸ªé€šé“åˆ†åˆ«è¿›è¡Œå·ç§¯è¿ç®—ï¼Œå¾—åˆ°å¤šé€šé“çš„ç‰¹å¾å›¾ï¼Œå†å¯¹åº”å…ƒç´ ç›¸åŠ äº§ç”Ÿå•ä¸ªå·ç§¯æ ¸çš„æœ€ç»ˆè¾“å‡ºã€‚\n",
    "\n",
    "åˆ†ç¦»å·ç§¯çš„è®¡ç®—æµç¨‹åˆ™ä¸åŒï¼Œå·ç§¯æ ¸çš„æ¯ä¸ªé€šé“ä¸è¾“å…¥çš„æ¯ä¸ªé€šé“è¿›è¡Œå·ç§¯è¿ç®—ï¼Œå¾—åˆ°å¤šä¸ªé€šé“çš„ä¸­é—´ç‰¹å¾ã€‚è¿™ä¸ªå¤šé€šé“çš„ä¸­é—´ç‰¹å¾å¼ é‡æ¥ä¸‹æ¥è¿›è¡Œå¤šä¸ª 1x1 å·é›†æ ¸çš„æ™®é€šå·ç§¯è¿ç®—ï¼Œå¾—åˆ°å¤šä¸ªé«˜å®½ä¸å˜çš„è¾“å‡ºï¼Œè¿™äº›è¾“å‡ºåœ¨é€šé“è½´ä¸Šé¢è¿›è¡Œæ‹¼æ¥ï¼Œä»è€Œäº§ç”Ÿæœ€ç»ˆçš„åˆ†ç¦»å·ç§¯å±‚çš„è¾“å‡ºã€‚åˆ†ç¦»å·ç§¯å±‚åŒ…å«äº†ä¸¤æ­¥å·ç§¯è¿ç®—ï¼Œç¬¬ä¸€æ­¥å·ç§¯è¿ç®—æ˜¯å•ä¸ªå·ç§¯æ ¸ï¼Œç¬¬äºŒä¸ªå·ç§¯è¿ç®—åŒ…å«äº†å¤šä¸ªå·ã€‚\n",
    "\n",
    "é‚£ä¹ˆé‡‡ç”¨åˆ†ç¦»å·ç§¯æœ‰ä»€ä¹ˆä¼˜åŠ¿å‘¢ï¼Ÿä¸€ä¸ªå¾ˆæ˜æ˜¾çš„ä¼˜åŠ¿åœ¨äºï¼ŒåŒæ ·çš„è¾“å…¥å’Œè¾“å‡ºï¼Œé‡‡ç”¨\n",
    "Separable Convolutionçš„å‚æ•°é‡çº¦æ˜¯æ™®é€šå·ç§¯çš„ä¸‰åˆ†ä¹‹ä¸€ã€‚\n",
    "\n",
    "æ¯”å¦‚ä¸‰é€šé“ï¼Œä¸‰é€šé“3x3çš„å·ç§¯æ ¸4ä¸ªï¼Œåˆ™å…¶å‚æ•°é‡ä¸º3x3x3x4=108ä¸ªã€‚\n",
    "\n",
    "è€Œåˆ†ç¦»å·ç§¯çš„ç¬¬ä¸€éƒ¨åˆ†ä¸º3x3x3x1=27ä¸ªï¼Œç¬¬äºŒéƒ¨åˆ†ä¸º1x1x3x4=14ä¸ªï¼Œæ€»å…±39ä¸ªã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. æ·±åº¦æ®‹å·®ç½‘ç»œ\n",
    "AlexNetï¼Œ VGGï¼Œ GoogLeNet ç­‰ç½‘ç»œæ¨¡å‹çš„å‡ºç°å°†ç¥ç»ç½‘ç»œçš„å‘å±•å¸¦å…¥äº†å‡ åå±‚çš„é˜¶æ®µï¼Œç ”ç©¶äººå‘˜å‘ç°ç½‘ç»œçš„å±‚æ•°è¶Šæ·±ï¼Œè¶Šæœ‰å¯èƒ½è·å¾—æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ä½†æ˜¯å½“æ¨¡å‹åŠ æ·±ä»¥åï¼Œç½‘ç»œå˜å¾—è¶Šæ¥è¶Šéš¾è®­ç»ƒï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ¢¯åº¦å¼¥æ•£ç°è±¡é€ æˆçš„ã€‚åœ¨è¾ƒæ·±å±‚æ•°çš„ç¥ç»ç½‘ç»œä¸­é—´ï¼Œæ¢¯åº¦ä¿¡æ¯ç”±ç½‘ç»œçš„æœ«å±‚é€å±‚ä¼ å‘ç½‘ç»œçš„é¦–å±‚æ—¶ï¼Œ ä¼ é€’çš„è¿‡ç¨‹ä¸­ä¼šå‡ºç°æ¢¯åº¦æ¥è¿‘äº 0 çš„ç°è±¡ã€‚ ç½‘ç»œå±‚æ•°è¶Šæ·±ï¼Œæ¢¯åº¦å¼¥æ•£ç°è±¡å¯èƒ½ä¼šè¶Šä¸¥é‡ã€‚\n",
    "\n",
    "é‚£ä¹ˆæ€ä¹ˆè§£å†³æ·±å±‚ç¥ç»ç½‘ç»œçš„æ¢¯åº¦å¼¥æ•£ç°è±¡å‘¢ï¼Ÿä¸€ä¸ªå¾ˆè‡ªç„¶çš„æƒ³æ³•æ˜¯ï¼Œæ—¢ç„¶æµ…å±‚ç¥ç»ç½‘ç»œä¸å®¹æ˜“å‡ºç°æ¢¯åº¦å¼¥æ•£ç°è±¡ï¼Œé‚£ä¹ˆå¯ä»¥å°è¯•ç»™æ·±å±‚ç¥ç»ç½‘ç»œæ·»åŠ ä¸€ç§å›é€€åˆ°æµ…å±‚ç¥ç»ç½‘ç»œçš„æœºåˆ¶ã€‚å½“æ·±å±‚ç¥ç»ç½‘ç»œå¯ä»¥è½»æ¾åœ°å›é€€åˆ°æµ…å±‚ç¥ç»ç½‘ç»œæ—¶ï¼Œæ·±å±‚ç¥ç»ç½‘ç»œå¯ä»¥è·å¾—ä¸æµ…å±‚ç¥ç»ç½‘ç»œç›¸å½“çš„æ¨¡å‹æ€§èƒ½ï¼Œè€Œä¸è‡³äºæ›´ç³Ÿç³•ã€‚\n",
    "\n",
    "é€šè¿‡åœ¨è¾“å…¥å’Œè¾“å‡ºä¹‹é—´æ·»åŠ ä¸€æ¡ç›´æ¥è¿æ¥çš„ Skip Connection å¯ä»¥è®©ç¥ç»ç½‘ç»œå…·æœ‰å›é€€çš„èƒ½åŠ›ã€‚ä»¥ VGG13 æ·±åº¦ç¥ç»ç½‘ç»œä¸ºä¾‹ï¼Œ å‡è®¾è§‚å¯Ÿåˆ° VGG13 æ¨¡å‹å‡ºç°æ¢¯åº¦å¼¥æ•£ç°è±¡ï¼Œè€Œ10 å±‚çš„ç½‘ç»œæ¨¡å‹å¹¶æ²¡æœ‰è§‚æµ‹åˆ°æ¢¯åº¦å¼¥æ•£ç°è±¡ï¼Œé‚£ä¹ˆå¯ä»¥è€ƒè™‘åœ¨æœ€åçš„ä¸¤ä¸ªå·ç§¯å±‚æ·»åŠ  SkipConnectionï¼Œå¦‚å›¾ 10.62 ä¸­æ‰€ç¤ºï¼š é€šè¿‡è¿™ç§æ–¹å¼ç½‘ç»œæ¨¡å‹å¯ä»¥è‡ªåŠ¨é€‰æ‹©æ˜¯å¦ç»ç”±è¿™ä¸¤ä¸ªå·ç§¯å±‚å®Œæˆç‰¹å¾å˜æ¢ï¼Œè¿˜æ˜¯ç›´æ¥è·³è¿‡è¿™ä¸¤ä¸ªå·ç§¯å±‚è€Œé€‰æ‹© Skip Connectionï¼Œäº¦æˆ–ç»“åˆä¸¤ä¸ªå·ç§¯å±‚å’Œ Skip Connection çš„è¾“å‡ºã€‚\n",
    "\n",
    "#### ResNetç®—æ³•\n",
    "ResNet é€šè¿‡åœ¨å·ç§¯å±‚çš„è¾“å…¥å’Œè¾“å‡ºä¹‹é—´æ·»åŠ  Skip Connection å®ç°å±‚æ•°å›é€€æœºåˆ¶ï¼Œè¾“å…¥ğ‘¥é€šè¿‡ä¸¤ä¸ªå·ç§¯å±‚ï¼Œå¾—åˆ°ç‰¹å¾å˜æ¢åçš„è¾“å‡ºâ„±(ğ‘¥)ï¼Œä¸è¾“å…¥ğ‘¥è¿›è¡Œå¯¹åº”å…ƒç´ çš„ç›¸åŠ è¿ç®—ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºã€‚\n",
    "\n",
    "H(ğ‘¥)å«åšæ®‹å·®æ¨¡å—ã€‚ç”±äºè¢« Skip Connection åŒ…å›´çš„å·ç§¯ç¥ç»ç½‘ç»œéœ€è¦å­¦ä¹ æ˜ å°„F(ğ‘¥) = H(ğ‘¥) âˆ’ ğ‘¥ï¼Œæ•…ç§°ä¸ºæ®‹å·®ç½‘ç»œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResBlockå®ç°\n",
    "from tensorflow.keras import layers\n",
    "class BasicBlock(layers.Layer):\n",
    "    def __init__(self, filter_num, stride = 1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # å·ç§¯å±‚1\n",
    "        self.conv1 = layers.Conv2D(filter_num, (3, 3), \n",
    "                                   strides = stride, padding = 'same'\n",
    "                                  )\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "        \n",
    "        # å·ç§¯å±‚2\n",
    "        self.conv1 = layers.Conv2D(filter_num, (3, 3), \n",
    "                                   strides = stride, padding = 'same'\n",
    "                                  )\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        \n",
    "        # å½“F(x)ä¸xå½¢çŠ¶ä¸åŒæ—¶æ— æ³•ç›´æ¥ç›¸åŠ \n",
    "        # éœ€è¦å»ºç«‹identity(x)å·ç§¯å±‚ï¼Œå®Œæˆxçš„å½¢çŠ¶è½¬æ¢\n",
    "        if stride != 1:\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, (1, 1), \n",
    "                                              strides = stride))\n",
    "        else:\n",
    "            self.downsample = lambda x : x\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # é€šè¿‡identity()è½¬æ¢\n",
    "        identity = self.downsample(inputs)\n",
    "        \n",
    "        # f(x) + xè¿ç®—\n",
    "        output = layers.add([out, identity])\n",
    "        \n",
    "        # é€šè¿‡æ¿€æ´»å‡½æ•°\n",
    "        output = tf.nn.relu(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. DenseNet\n",
    "DenseNet å°†å‰é¢æ‰€æœ‰å±‚çš„ç‰¹å¾å›¾ä¿¡æ¯é€šè¿‡ Skip Connection ä¸å½“å‰å±‚è¾“å‡ºè¿›è¡Œèšåˆï¼Œä¸ResNet çš„å¯¹åº”ä½ç½®ç›¸åŠ ä¸åŒï¼Œ DenseNet é‡‡ç”¨åœ¨é€šé“è½´ c ç»´åº¦è¿›è¡Œæ‹¼æ¥æ“ä½œï¼Œ èšåˆç‰¹å¾ä¿¡æ¯ã€‚\n",
    "\n",
    "è¾“å…¥ğ‘¥0é€šè¿‡H1å·ç§¯å±‚å¾—åˆ°è¾“å‡ºğ‘¥1ï¼Œ ğ‘¥1ä¸ğ‘¥0åœ¨é€šé“è½´ä¸Šè¿›è¡Œæ‹¼æ¥ï¼Œå¾—åˆ°èšåˆåçš„ç‰¹å¾å¼ é‡ï¼Œé€å…¥H2å·ç§¯å±‚ï¼Œå¾—åˆ°è¾“å‡ºğ‘¥2ï¼ŒåŒæ ·çš„æ–¹æ³•ï¼Œ ğ‘¥2ä¸å‰é¢æ‰€æœ‰å±‚çš„ç‰¹å¾ä¿¡æ¯: ğ‘¥1ä¸ğ‘¥0è¿›è¡Œèšåˆï¼Œå†é€å…¥ä¸‹ä¸€å±‚ã€‚å¦‚æ­¤å¾ªç¯ï¼Œç›´è‡³æœ€åä¸€å±‚çš„è¾“å‡ºğ‘¥4å’Œå‰é¢æ‰€æœ‰å±‚çš„ç‰¹å¾ä¿¡æ¯ï¼š {ğ‘¥ğ‘–}ğ‘–=0 1 2 3è¿›è¡Œèšåˆå¾—åˆ°æ¨¡å—çš„æœ€ç»ˆè¾“å‡ºã€‚è¿™æ ·ä¸€ç§åŸºäº Skip Connection ç¨ å¯†è¿æ¥çš„æ¨¡å—å«åš Dense Blockã€‚\n",
    "\n",
    "### 13. CIFAR10å’ŒResNet18å®æˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(layers.Layer):\n",
    "    \n",
    "    # æ®‹å·®æ¨¡å—\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # ç¬¬ä¸€ä¸ªå·ç§¯å•å…ƒ\n",
    "        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride,\n",
    "                                   padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "        \n",
    "        # ç¬¬äºŒä¸ªå·ç§¯å•å…ƒ\n",
    "        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1,\n",
    "                                   padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        # é€šè¿‡ 1x1 å·ç§¯å®Œæˆ shape åŒ¹é…\n",
    "        if stride != 1:\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, (1, 1),\n",
    "                                strides=stride))\n",
    "        # shape åŒ¹é…ï¼Œç›´æ¥çŸ­æ¥\n",
    "        else:\n",
    "            self.downsample = lambda x:x\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "    \n",
    "        # [b, h, w, c]ï¼Œé€šè¿‡ç¬¬ä¸€ä¸ªå·ç§¯å•å…ƒ\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # é€šè¿‡ç¬¬äºŒä¸ªå·ç§¯å•å…ƒ\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # é€šè¿‡ identity æ¨¡å—\n",
    "        identity = self.downsample(inputs)\n",
    "        \n",
    "        # 2 æ¡è·¯å¾„è¾“å‡ºç›´æ¥ç›¸åŠ \n",
    "        output = layers.add([out, identity])\n",
    "        \n",
    "        # æ¿€æ´»å‡½æ•°\n",
    "        output = tf.nn.relu(output) \n",
    "        return output\n",
    "    \n",
    "    # å¯ä»¥é€šè¿‡å †å é€šé“æ•°é€æ¸å¢å¤§çš„ Res Block æ¥å®ç°é«˜å±‚ç‰¹å¾çš„æå–\n",
    "    # é€šè¿‡ build_resblock å¯ä»¥ä¸€æ¬¡å®Œæˆå¤šä¸ªæ®‹å·®æ¨¡å—çš„æ–°å»º\n",
    "    def build_resblock(self, filter_num, blocks, stride=1):\n",
    "    \n",
    "        # è¾…åŠ©å‡½æ•°ï¼Œå †å  filter_num ä¸ª BasicBlock\n",
    "        res_blocks = Sequential()\n",
    "        \n",
    "        # åªæœ‰ç¬¬ä¸€ä¸ª BasicBlock çš„æ­¥é•¿å¯èƒ½ä¸ä¸º 1ï¼Œå®ç°ä¸‹é‡‡æ ·\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "        \n",
    "        #å…¶ä»– BasicBlock æ­¥é•¿éƒ½ä¸º 1\n",
    "        for _ in range(1, blocks):\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    "        \n",
    "        return res_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€šç”¨çš„ ResNet å®ç°ç±»\n",
    "class ResNet(tf.keras.Model):\n",
    "    \n",
    "    # [2, 2, 2, 2]\n",
    "    def __init__(self, layer_dims, num_classes=10): \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # æ ¹ç½‘ç»œï¼Œé¢„å¤„ç†\n",
    "        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),\n",
    "                                layers.BatchNormalization(),\n",
    "                                layers.Activation('relu'),\n",
    "                                layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1),\n",
    "                                                 padding='same')\n",
    "        ])\n",
    "        # å †å  4 ä¸ª Blockï¼Œæ¯ä¸ª block åŒ…å«äº†å¤šä¸ª BasicBlock,è®¾ç½®æ­¥é•¿ä¸ä¸€æ ·\n",
    "        self.layer1 = self.build_resblock(64, layer_dims[0])\n",
    "        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)\n",
    "        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)\n",
    "        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)\n",
    "        \n",
    "        # é€šè¿‡ Pooling å±‚å°†é«˜å®½é™ä½ä¸º 1x1\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        \n",
    "        # æœ€åè¿æ¥ä¸€ä¸ªå…¨è¿æ¥å±‚åˆ†ç±»\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "    \n",
    "        # å¯ä»¥é€šè¿‡å †å é€šé“æ•°é€æ¸å¢å¤§çš„ Res Block æ¥å®ç°é«˜å±‚ç‰¹å¾çš„æå–\n",
    "    # é€šè¿‡ build_resblock å¯ä»¥ä¸€æ¬¡å®Œæˆå¤šä¸ªæ®‹å·®æ¨¡å—çš„æ–°å»º\n",
    "    def build_resblock(self, filter_num, blocks, stride=1):\n",
    "    \n",
    "        # è¾…åŠ©å‡½æ•°ï¼Œå †å  filter_num ä¸ª BasicBlock\n",
    "        res_blocks = Sequential()\n",
    "        \n",
    "        # åªæœ‰ç¬¬ä¸€ä¸ª BasicBlock çš„æ­¥é•¿å¯èƒ½ä¸ä¸º 1ï¼Œå®ç°ä¸‹é‡‡æ ·\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "        \n",
    "        #å…¶ä»– BasicBlock æ­¥é•¿éƒ½ä¸º 1\n",
    "        for _ in range(1, blocks):\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    "        \n",
    "        return res_blocks\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # é€šè¿‡æ ¹ç½‘ç»œ\n",
    "        x = self.stem(inputs)\n",
    "        \n",
    "        # ä¸€æ¬¡é€šè¿‡ 4 ä¸ªæ¨¡å—\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # é€šè¿‡æ± åŒ–å±‚\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        # é€šè¿‡å…¨è¿æ¥å±‚\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€šè¿‡è°ƒæ•´æ¨¡å—å†…éƒ¨ BasicBlock çš„æ•°é‡å’Œé…ç½®å®ç°ä¸åŒçš„ ResNet\n",
    "def resnet18():\n",
    "    return ResNet([2, 2, 2, 2])\n",
    "\n",
    "def resnet34():\n",
    "    return ResNet([3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®é›†\n",
    "(x,y), (x_test, y_test) = tf.keras.datasets.cifar10.load_data() \n",
    "\n",
    "# åˆ é™¤ä¸å¿…è¦çš„ç»´åº¦\n",
    "y = tf.squeeze(y, axis=1) \n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„å¤„ç†\n",
    "def preprocess(x, y):\n",
    "    \n",
    "    # å°†æ•°æ®æ˜ å°„åˆ°-1~1\n",
    "    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    \n",
    "    # ç±»å‹è½¬æ¢\n",
    "    y = tf.cast(y, dtype=tf.int32) \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºè®­ç»ƒé›†\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "\n",
    "# éšæœºæ‰“æ•£ï¼Œé¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(512)\n",
    "\n",
    "#æ„å»ºæµ‹è¯•é›†\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \n",
    "\n",
    "# éšæœºæ‰“æ•£ï¼Œé¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–\n",
    "test_db = test_db.map(preprocess).batch(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()\n",
    "\n",
    "# è®­ç»ƒ epoch\n",
    "for epoch in range(50): \n",
    "    \n",
    "    for step, (x,y) in enumerate(train_db):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # [b, 32, 32, 3] => [b, 10],å‰å‘ä¼ æ’­\n",
    "            logits = model(x)\n",
    "            \n",
    "            # [b] => [b, 10],one-hot ç¼–ç \n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            \n",
    "            # è®¡ç®—äº¤å‰ç†µ\n",
    "            loss = tf.losses.categorical_crossentropy(y_onehot, logits,\n",
    "                                                      from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            \n",
    "        # è®¡ç®—æ¢¯åº¦ä¿¡æ¯\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # æ›´æ–°ç½‘ç»œå‚æ•°\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
